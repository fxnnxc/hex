{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set GPU ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/cwkang/anaconda3/envs/attention_intervention/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "print('hi')\n",
    "\n",
    "import torch\n",
    "torch.cuda.set_device(7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HANS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset hans (/home/cwkang/.cache/huggingface/datasets/hans/plain_text/1.0.0/452e93cf5383f5ae39088254215b517d0da98ccaaf0af8f7ab04d8f23f67dbd9)\n",
      "100%|██████████| 2/2 [00:00<00:00, 453.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = load_dataset('hans')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer):\n",
    "    max_seq_length = min(128, tokenizer.model_max_length)\n",
    "    sentence1_key, sentence2_key = (\"premise\", \"hypothesis\")\n",
    "    label_to_id = {0:0, 1:1, 2:2}\n",
    "\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=False, max_length=max_seq_length, truncation=True)\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    # for k, v in result.items():\n",
    "    #     result[k] = torch.tensor(v).to('cuda')\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "# model_name_or_path = \"results/mnli/bert-base-uncased\"\n",
    "model_name_or_path = \"ishan/bert-base-uncased-mnli\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=3,\n",
    "    finetuning_task='mnli',\n",
    "    use_auth_token=None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    use_auth_token=None,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "    config=config,\n",
    "    use_auth_token=None,\n",
    "    ignore_mismatched_sizes=False,\n",
    ")\n",
    "\n",
    "# import torch\n",
    "# if hasattr(torch, 'compile'):\n",
    "#     model = torch.compile(model)\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cwkang/.cache/huggingface/datasets/hans/plain_text/1.0.0/452e93cf5383f5ae39088254215b517d0da98ccaaf0af8f7ab04d8f23f67dbd9/cache-e02ed34140c2f999.arrow\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "eval_dataset = dataset[\"validation\"].map(\n",
    "    partial(preprocess_function, tokenizer=tokenizer),\n",
    "    batched=True,\n",
    "    # remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model on HANS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [07:27<00:00, 67.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# mnli_label_dict = ['entailment', 'neutral', 'contradiction']\n",
    "mnli_label_dict = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "hans_label_dict = ['entailment', 'nonentailment']\n",
    "\n",
    "e_correct = 0\n",
    "n_correct = 0\n",
    "e_total = 0\n",
    "n_total = 0\n",
    "\n",
    "attention_dict = {}\n",
    "key_count = defaultdict(int)\n",
    "for inputs in tqdm(eval_dataset):\n",
    "    sample = tokenizer.pad(\n",
    "        inputs,\n",
    "        padding=\"max_length\",\n",
    "        max_length=min(128, tokenizer.model_max_length),\n",
    "        pad_to_multiple_of=None,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if \"label\" in sample:\n",
    "        sample[\"labels\"] = sample[\"label\"]\n",
    "        del sample[\"label\"]\n",
    "\n",
    "    inputs = {k: torch.tensor([v]).to(\"cuda\") for k, v in sample.items() if k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]}\n",
    "    output = model(**inputs, output_attentions=True)\n",
    "    predictions = output.logits.argmax(dim=-1)\n",
    "    predictions_ = mnli_label_dict[predictions]\n",
    "    references = sample['labels']\n",
    "    references_ = hans_label_dict[references]\n",
    "\n",
    "    key = sample[\"template\"] + \"_\" + predictions_\n",
    "    if key not in attention_dict:\n",
    "        attention_dict[key] = []\n",
    "        for layer_idx, weights in enumerate(output.attentions):\n",
    "            attention_dict[key].append(weights.detach().cpu())\n",
    "    else:\n",
    "        for layer_idx, weights in enumerate(output.attentions):\n",
    "            attention_dict[key][layer_idx] = attention_dict[key][layer_idx] + weights.detach().cpu()\n",
    "    key_count[key] += 1\n",
    "\n",
    "    if references_ == \"entailment\":\n",
    "        e_total += 1\n",
    "        if predictions_ == \"entailment\":\n",
    "            e_correct += 1\n",
    "    else:\n",
    "        n_total += 1\n",
    "        if predictions_ != \"entailment\":\n",
    "            n_correct += 1\n",
    "\n",
    "    # print(sample, '\\n')\n",
    "    # print('pred:', predictions.item(), predictions_)\n",
    "    # print('answer:', references, references_)\n",
    "\n",
    "    # print(torch.tensor(output.attentions).shape)\n",
    "    # print(output.attentions[0].shape)\n",
    "    # print(len(output.attentions))\n",
    "\n",
    "for key in attention_dict:\n",
    "    for layer_idx, weights in enumerate(output.attentions):\n",
    "        attention_dict[key][layer_idx] /= key_count[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14850 15000\n",
      "2165 15000\n"
     ]
    }
   ],
   "source": [
    "print(e_correct, e_total)\n",
    "print(n_correct, n_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention intervention with hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intervention_hook.intervention import bert_attention_intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in attention_dict:\n",
    "    for layer_idx, weights in enumerate(output.attentions):\n",
    "        attention_dict[key][layer_idx] = attention_dict[key][layer_idx].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [06:54<00:00, 72.31it/s]\n"
     ]
    }
   ],
   "source": [
    "intervention_e_correct = 0\n",
    "intervention_n_correct = 0\n",
    "intervention_e_total = 0\n",
    "intervention_n_total = 0\n",
    "\n",
    "intervention_count = 0\n",
    "for inputs in tqdm(eval_dataset):\n",
    "    sample = tokenizer.pad(\n",
    "        inputs,\n",
    "        padding=\"max_length\",\n",
    "        max_length=min(128, tokenizer.model_max_length),\n",
    "        pad_to_multiple_of=None,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if \"label\" in sample:\n",
    "        sample[\"labels\"] = sample[\"label\"]\n",
    "        del sample[\"label\"]\n",
    "\n",
    "    inputs = {k: torch.tensor([v]).to(\"cuda\") for k, v in sample.items() if k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]}\n",
    "\n",
    "    references = sample['labels']\n",
    "    references_ = hans_label_dict[references]\n",
    "    keys = []\n",
    "    if references_ == \"nonentailment\":\n",
    "        if sample[\"template\"] + \"_contradiction\" in attention_dict:\n",
    "            keys.append(sample[\"template\"] + \"_contradiction\")\n",
    "        if sample[\"template\"] + \"_neutral\" in attention_dict:\n",
    "            keys.append(sample[\"template\"] + \"_neutral\")\n",
    "    elif sample[\"template\"] + \"_entailment\" in attention_dict:\n",
    "        keys.append(sample[\"template\"] + \"_entailment\")\n",
    "    key = keys[0] if len(keys) > 0 else None\n",
    "\n",
    "    if key is not None:\n",
    "        output = bert_attention_intervention(model, inputs, attention_dict[key])\n",
    "        intervention_count += 1\n",
    "    else:\n",
    "        output = model(**inputs, output_attentions=True)\n",
    "\n",
    "    predictions = output.logits.argmax(dim=-1)\n",
    "    predictions_ = mnli_label_dict[predictions]\n",
    "\n",
    "    if references_ == \"entailment\":\n",
    "        intervention_e_total += 1\n",
    "        if predictions_ == \"entailment\":\n",
    "            intervention_e_correct += 1\n",
    "    else:\n",
    "        intervention_n_total += 1\n",
    "        if predictions_ != \"entailment\":\n",
    "            intervention_n_correct += 1\n",
    "\n",
    "    # print(sample, '\\n')\n",
    "    # print('pred:', predictions.item(), predictions_)\n",
    "    # print('answer:', references, references_)\n",
    "\n",
    "    # print(torch.tensor(output.attentions).shape)\n",
    "    # print(output.attentions[0].shape)\n",
    "    # print(len(output.attentions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29034\n"
     ]
    }
   ],
   "source": [
    "print(intervention_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14440 15000\n",
      "6517 15000\n"
     ]
    }
   ],
   "source": [
    "print(intervention_e_correct, intervention_e_total)\n",
    "print(intervention_n_correct, intervention_n_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention_intervention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e3625c4dcb7c2231d618852f95daca40f4f087b7d7a6d2d8f9dcb0d8f11a3cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
